{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Tweets\n",
    "\n",
    "In this notebook, we will delve into the analysis of tweet contents.\n",
    "\n",
    "We consider the dataset of tweets from Elon Musk, SpaceX and Tesla founder, and ask the following questions:\n",
    "* What is Elon most actively tweeting about?\n",
    "* Who is Elon most frequently referring to?\n",
    "\n",
    "We will explore how to work with the contents of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os, codecs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some basics (or a refresher) of working with texts in Python. Texts are sequences of discrete symbols (words or, more generically, tokens)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the dataset\n",
    "Let us import the Elon Musk's tweets dataset in memory.\n",
    "\n",
    "<img src=\"images/elon_loop.jpeg\" width=\"400px\" heigth=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dataset using Pandas, and create a data frame\n",
    "\n",
    "root_folder = \"data\"\n",
    "df_elon = pd.read_csv(codecs.open(os.path.join(root_folder,\"elonmusk_tweets.csv\"), encoding=\"utf8\"), sep=\",\")\n",
    "df_elon['text'] = df_elon['text'].str[1:] #remove the starting 'b' from every tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elon.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elon.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elon.shape # (number of rows, number of columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_elon[\"text\"].tolist()[:10] #convert a column to a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with tweet contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some of the most popular libraries for NLP in Python\n",
    "import nltk\n",
    "import string\n",
    "#import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical NLP pipeline might look like the following:\n",
    "    \n",
    "<img src=\"images/spacy_pipeline.png\" width=\"600px\" heigth=\"600px\">\n",
    "\n",
    "* Tokenization: split a text into tokens.\n",
    "* Filtering: remove some of the tokens if not needed (e.g., punctuation). If and how to remove is task dependent.\n",
    "* Tagger, parser: syntactic structure.\n",
    "* NER (Named Entity Recognition): find named entities.\n",
    "* ...\n",
    "\n",
    "### Tokenization: splitting a text into constituent tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK provides us with a tokenizers for tweets\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer, word_tokenize\n",
    "tknzr = TweetTokenizer(preserve_case=True, reduce_len=False, strip_handles=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tweet = df_elon.text[1]\n",
    "print(example_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tokenizer takes a string and outputs a list of tokens.\n",
    "\n",
    "We compare here two tokenizers: one for general English texts, and one specialized for tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkz1 = tknzr.tokenize(example_tweet)\n",
    "print(tkz1)\n",
    "print(\"\\n======\\n\")\n",
    "tkz2 = word_tokenize(example_tweet)\n",
    "print(tkz2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: can you spot what the Twitter tokenizer is doing instead of a standard one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering unnecessary tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some more pre-processing\n",
    "\n",
    "def filter_twt(tweet):\n",
    "    \n",
    "    # remove punctuation and short words and urls\n",
    "    tweet = [t for t in tweet if t not in string.punctuation and len(t) > 3 and not t.startswith(\"http\") and not t.startswith(\"www\")]\n",
    "    return tweet\n",
    "\n",
    "def tokenize_and_string(tweet):\n",
    "    \n",
    "    tkz = tknzr.tokenize(tweet)\n",
    "    \n",
    "    tkz = filter_twt(tkz)\n",
    "    \n",
    "    return \" \".join(tkz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tkz1)\n",
    "print(\"======\")\n",
    "print(filter_twt(tkz1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elon[\"clean_text\"] = df_elon[\"text\"].apply(tokenize_and_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elon.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned up version\n",
    "\n",
    "#df_elon.to_csv(os.path.join(root_folder,\"df_elon.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a dictionary with word occurrences\n",
    "\n",
    "We want to build a dictionary of unique tokens, containing the number of times they appear in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_tokens = list()\n",
    "for tweet in df_elon[\"clean_text\"].tolist():\n",
    "    all_tokens.extend(tweet.split())\n",
    "\n",
    "c = Counter(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "* Find the tokens most used by Elon.\n",
    "* Find the Twitter users most referred to by Elon (hint: use the @ handler to spot them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[d for d in c.most_common(1000) if not d[0].startswith('@')][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[d for d in c.most_common(1000) if d[0].startswith('@')][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization\n",
    "\n",
    "The `pandas`' API provides integration with the plotting functionalities provided by the `matplotlib` library.\n",
    "\n",
    "This seamless integration – which is very nice! – hides away from users some of the complexities of `matplotlib`.\n",
    "\n",
    "However, as there cases where advanced customizations are needed, it's useful to learn the high-level plotting functionalities of `pandas` or `seaborn` as well as being aware of how to perform more advanced customizations by means of `matplotlib`.\n",
    "\n",
    "Very useful [`matplotlib` cheatsheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Python_Matplotlib_Cheat_Sheet.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the number of tweets mentioning one of the top 10 tokens over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the created_at column to datetime\n",
    "\n",
    "df_elon.created_at = pd.to_datetime(df_elon.created_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elon[\"year\"] = df_elon.created_at.dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of tweets containing a certain word (or user name)\n",
    "\n",
    "which_word = '@SpaceX' #Tesla\n",
    "\n",
    "df_elon[\"word_in_tweet\"] = df_elon.clean_text.apply(lambda x: which_word in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = df_elon.groupby('year').word_in_tweet.agg('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(d.index,d.values,color=\"skyblue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(d.index,d.values,color=\"skyblue\")\n",
    "plt.xlabel(\"Year\", fontsize=14)\n",
    "plt.ylabel(\"Count\", fontsize=14)\n",
    "plt.title(\"Number of tweets mentioning %s\"%which_word, fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"stuff/elon_plot.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another question: how many tweets per month over time? We need to change the index and group.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Use seaborn style defaults and set the default figure size\n",
    "sns.set(rc={'figure.figsize':(11, 4)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elon.index = pd.to_datetime(df_elon['created_at'],format='%m/%d/%y %I:%M%p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elon.groupby(pd.Grouper(freq='M')).agg('count')['id'][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elon.groupby(pd.Grouper(freq='M')).agg('count')['id'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**: there is much more to this than plotting. Take a loot at the [Seaborn](https://seaborn.pydata.org/examples/index.html) or [Matplotlib](https://matplotlib.org/gallery.html) galleries for some compelling examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anatomy of a plot (OPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we create the figure, which is the \n",
    "# container where all plots reside\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "ax1 = fig.add_subplot(2, 2, 1)\n",
    "plt.plot(np.random.randn(50).cumsum(), 'k--')\n",
    "\n",
    "ax2 = fig.add_subplot(2, 2, 2)\n",
    "ax3 = fig.add_subplot(2, 2, 3)\n",
    "ax4 = fig.add_subplot(2, 2, 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# first we creta the figure, which is the \n",
    "# container where all plots reside\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "ax1 = fig.add_subplot(2, 2, 1)\n",
    "plt.plot(np.random.randn(50), 'k--')\n",
    "\n",
    "ax2 = fig.add_subplot(2, 2, 2)\n",
    "ax3 = fig.add_subplot(2, 2, 3)\n",
    "ax4 = fig.add_subplot(2, 2, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each plot resides within a `Figure` object.\n",
    "\n",
    "Each subsplot resides within an `AxesSubplot` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2)\n",
    "axes[0,1].plot(np.random.randn(50), 'r--')\n",
    "axes[0,1].plot(np.random.randn(50), 'b--')\n",
    "axes[1,1].plot(np.random.randn(50), 'k--')\n",
    "axes[1,0].plot(np.random.randn(50), '.')\n",
    "axes[0,0].plot(np.random.randn(50), 'y-')\n",
    "fig.set_size_inches(10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.\n",
    "\n",
    "* Plot the top n words together in a single figure, and show their trends over time.\n",
    "* Do the same for the top n users mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
